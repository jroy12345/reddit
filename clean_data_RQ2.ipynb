{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad118feb-43c0-4c2d-b326-10bf215b5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "import ast\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f2bb2",
   "metadata": {},
   "source": [
    "## File Overview\n",
    "\n",
    "RQ2 involves tracking the toxicity levels of the subreddit over time.\n",
    "\n",
    "In order to do so, we will use the [lexicon dictionaries](https://github.com/miriamfs/WebSci2019) from the paper: \"Exploring Misogyny across the Manosphere in Reddit. In: WebSci '19 Proceedings of the 10th ACM Conference on Web Science\" by Farrell et al. \n",
    "\n",
    "We will compare the comments in the collected dataset to the lexicon dictionaries. We will see how often the comments contain words that are a part of the dictionaries. This will allow us to see the impact of the quarantine on toxicity. \n",
    "\n",
    "We must first prepare the data.\n",
    "\n",
    "The overall process is as follows: \n",
    "\n",
    "    1. Load the lexicon dictionaries\n",
    "    2. Tokenize the comments\n",
    "    3. Count frequency of dictionary terms in comments dataset\n",
    "    4. Convert counts to a usable dataframe\n",
    "    5. Merge count dataframe with comments dataset\n",
    "    6. Export dataframe to be used in analysis/visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d4b27",
   "metadata": {},
   "source": [
    "###  1. Load the lexicon dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87bb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dict of dictionary number: dictionary names from lexicon dictionary\n",
    "file = open(\"Lexicon_names.txt\", \"r\")\n",
    "contents = file.read()\n",
    "lexicon_names = ast.literal_eval(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cda474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dict of dictionary number: words in dictionary from lexicon dictionary\n",
    "\n",
    "file = open(\"Lexicon_values.txt\", \"r\")\n",
    "contents = file.read()\n",
    "lexicon_values = ast.literal_eval(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eadc789",
   "metadata": {},
   "source": [
    "### 2. Tokenize the comments, 3. Count frequency of dictionary terms in comments dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd5aa7-bcf2-4873-b990-329e17bd4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read filtered comments data\n",
    "filtered_comments = pd.read_json('filtered_comments.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131bbeb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the following code block gets the number of words in each reddit\n",
    "# comment that also appears in each of the 9 lexicon dictionaries\n",
    "\n",
    "counts=[]\n",
    "\n",
    "# for each comment, tokenize to the word level\n",
    "for row in filtered_comments['body']:\n",
    "    cnt=Counter()\n",
    "    comment = word_tokenize(row)\n",
    "    \n",
    "    # for each word, see if it is in each dictionary and increment the counter \n",
    "    for word in comment:\n",
    "        for key, value in lexicon_values.items():\n",
    "            if word in value:\n",
    "                cnt[key] += 1\n",
    "                \n",
    "    # save the count for each row to be merged later         \n",
    "    counts.append(cnt)\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c0fdf",
   "metadata": {},
   "source": [
    "### 4. Convert counts to a usable dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the list of counts into a dataframe using json \n",
    "count_test = json.dumps(counts)\n",
    "count_df = pd.read_json(count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganize the counted data to be better merge with filtered_comments\n",
    "\n",
    "count_df.sort_index(ascending=True, axis=1, inplace=True)\n",
    "count_df.reset_index(inplace=True)\n",
    "\n",
    "# rename the lexicon dictionary numbers to the dictionary names\n",
    "count_df.rename(mapper=lexicon_names, axis=1, inplace=True)\n",
    "count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec996e54",
   "metadata": {},
   "source": [
    "### 5. Merge count dataframe with comments dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d26d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt the counts df to better merge with the comments df\n",
    "# longer df is better than wider df for analysis\n",
    "\n",
    "value_columns = count_df.columns[1:]\n",
    "counted = pd.melt(count_df, id_vars=['index'], value_vars=value_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null values with 0 to be used in analysis\n",
    "counted.fillna({'value': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0910dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "counted.groupby('variable')[['value']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered_comments ready for merge and then merge\n",
    "filtered_comments.reset_index(inplace=True)\n",
    "\n",
    "comments_rq2 = filtered_comments.merge(right=counted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1a1223",
   "metadata": {},
   "source": [
    "### 6. Export dataframe to be used in analysis/visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_rq2.to_json('./comments_rq2.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
